---
locales:
  - en_US.UTF-8
  - de_DE.UTF-8
locale: en_US.UTF-8
system_timezone: "Europe/Berlin"
pip_package: python3-pip
pip_install_packages:
  - name: pyyaml==6.0.1
  - name: docker==6.1.3
ip_prefix: 192.168.50.
router_ip: 192.168.50.1
nameserver_pi_ip: "{{ hostvars['nameserver-pi'].ansible_host }}"
# virtualhere_pi_ip: "{{ hostvars['virtualhere-pi'].ansible_host }}"
homelab_nuc_ip: "{{ hostvars['homelab-nuc'].ansible_host }}"
ailab_ip: "{{ hostvars['ailab'].ansible_host }}"
ailab_mac_address: 50:eb:f6:82:3c:38 # MAC address for Wake-on-LAN
ailab_ubuntu_ip: 192.168.50.10
ailab_proxmox_ip: "{{ hostvars['ailab-proxmox'].ansible_host }}"
micpi_ip: "{{ hostvars['micpi'].ansible_host }}"
cidr: 192.168.50.0/24
pihole_http_port: 81
portainer_http_port: 9002
glances_http_port: 61208
homeassistant_http_port: 8123
homepage_http_port: 3002
music_assistant_http_port: 8095
uptime_kuma_http_port: 3001
wud_http_port: 3000
semaphore_http_port: 3005
vscode_http_port: 8443
nodered_http_port: 1880
mosquitto_address: "{{ ansible_default_ipv4.address }}"
mosquitto_port: 1883
mosquitto_websockets_port: 9001
z2m_http_port: 8080
frigate_http_port: 5000
jellyfin_http_port: 8096
navidrome_http_port: 4533
teslamate_http_port: 8098
grafana_http_port: 3004
syncthing_http_port: 8384
ring_mqtt_rtsp_port: 8556
open_webui_http_port: 3123
comfyui_port: 8188
ollama_port: 11434 # Default Ollama API port
llamaswap_port: 9292 # llama-swap API port
llamaswap_container_name: llama-swap
llamaswap_config_path: "{{ config_root }}/llama-swap"
model_group_alias:
  home: "GPT-OSS-20B-F16"
  home-fast: "GPT-OSS-20B-F16-low"
  home-smart: "GPT-OSS-20B-F16-high"
  home-vision: "Qwen3-VL-8B-Instruct"
llamaswap_models:
  - alias: Qwen3-VL-8B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:Q8_K_XL
      --port ${PORT}
      --ctx-size 20000
      --flash-attn auto
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --top-p 0.8
      --top-k 20
      --temp 0.7
      --min-p 0.0
      --presence-penalty 1.5
      --jinja
    supports_function_calling: true
    supports_vision: true
    group: swap
  - alias: Qwen3-VL-30B-A3B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF:Q4_K_XL
      --port ${PORT}
      --ctx-size 32768
      --n-predict 32768
      --flash-attn auto
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      -ot ".ffn_.*_exps.=CPU"
      --top-p 0.8
      --top-k 20
      --temp 0.7
      --min-p 0.0
      --presence-penalty 1.5
      --jinja
    supports_function_calling: true
    supports_vision: true
    group: swap
  - alias: Qwen3-VL-30B-A3B-Thinking
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF:Q4_K_XL
      --port ${PORT}
      --ctx-size 40960
      --n-predict 40960
      --flash-attn auto
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      -ot ".ffn_.*_exps.=CPU"
      --top-p 0.95
      --top-k 20
      --temp 1.0
      --min-p 0.0
      --presence-penalty 0.0
      --jinja
    supports_function_calling: true
    supports_vision: true
    group: swap
  - alias: Qwen3-Coder-30B-A3B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q4_K_XL
      --port ${PORT}
      --ctx-size 32684
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      -ot ".ffn_.*_exps.=CPU"
      --threads -1
      --temp 0.7
      --min-p 0.0
      --top-p 0.80
      --top-k 20
      --repeat-penalty 1.05
      --jinja
    supports_function_calling: true
    supports_vision: false
    group: swap
  - alias: Qwen3-30B-A3B-Instruct-2507
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q4_K_XL
      -hfd unsloth/Qwen3-0.6B-GGUF:Q4_K_XL
      --port ${PORT}
      --ctx-size 24000
      --flash-attn auto
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --threads -1
      --batch-size 512
      --ubatch-size 512
      --temp 0.7
      --min-p 0.0
      --top-p 0.8
      --top-k 20
      --presence-penalty 1.0
      --jinja
    supports_function_calling: true
    supports_vision: false
    group: swap
  - alias: Mistral-Small-3.2-24B-Instruct-2506
    cmd: >-
      /app/llama-server
      -hf unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF:UD-Q4_K_XL
      --port ${PORT}
      --ctx-size 32000
      --flash-attn auto
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --temp 0.15
      --top-p 1.0
      --top-k -1
      --jinja
    supports_function_calling: true
    supports_vision: true
    group: swap
  - alias: GPT-OSS-20B-F16
    cmd: >-
      /app/llama-server
      -hf unsloth/gpt-oss-20b-GGUF:F16
      --port ${PORT}
      --ctx-size 131072
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --threads 4
      --temp 1.0
      --top-p 1.0
      --top-k 0
      --jinja
    supports_function_calling: true
    supports_vision: false
    group: swap
    variants:
      high:
        chat_template_kwargs:
          reasoning_effort: "high"
      low:
        chat_template_kwargs:
          reasoning_effort: "low"
  - alias: Qwen3-0.6B-Q4
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-0.6B-GGUF:Q4_K_XL
      --port ${PORT}
      --ctx-size 4096
      --flash-attn auto
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --temp 0.7
      --top-p 0.8
      --top-k 20
      --min-p 0.0
      --chat_template_kwargs '{\"enable_thinking\": false}'
      --jinja
    supports_function_calling: true
    supports_vision: false
    group: forever

llamaswap_model_groups:
  forever:
    persistent: false
    swap: false
    exclusive: false
  swap:
    swap: true

babybuddy_http_port: 8008
wallabag_http_port: 8010
readeck_http_port: 8011
linkding_http_port: 8012
n8n_http_port: 5678
teddycloud_http_port: 8013
teddycloud_box_port: 4443
metamcp_port_host: 8015
changedetection_http_port: 8016
immich_http_port: 2283
immich_ml_http_port: 3003
immich_ml_hostname: "ailab-ubuntu.lan"
immich_ml_port: "{{ immich_ml_http_port }}"
immich_db_name: "immich"
immich_db_username: "postgres"
immich_db_password: "test123"  # Shared password for split deployment
immich_db_hostname: "192.168.50.5"  # homelab-nuc IP
immich_db_port: 5432
immich_redis_hostname: "192.168.50.5"  # homelab-nuc IP
immich_redis_port: 6379
immich_ml_gpu_enabled: true
open_webui_config_folder: "{{ config_root }}/open-webui"
sunshine_address: 192.168.50.33
sunshine_http_port: 47990
uid: 1000 # TODO move to host_vars
username_personal: daniel
uid_personal: 1001
mount_root: mnt
shares:
  - medien
  - config
  - ai-models
timemachine_shares:
  - timemachine
dataset_shares: "{{ shares }}"
syncthing_datasets:
  - medien
  - ai-models
config_root: /home/{{ username }}/config
base_src_path: /opt/src # Base path for cloning source repos
ollama_host: ailab-ubuntu.lan
litellm_proxy_port: 4000
litellm_proxy_ui_port: 4001
langfuse_port: 3003
homeassistant_config_folder: "{{ config_root }}/home-assistant"
homeassistant_media_folder: /tank/medien
music_assistant_data_folder: "{{ config_root }}/music-assistant"
music_assistant_media_folder: /tank/medien/Musik
frigate_config_folder: "{{ config_root }}/frigate"
frigate_media_folder: /tank/medien/Frigate
vscode_config_folder: "{{ config_root }}/vscode"
nodered_config_folder: "{{ config_root }}/node-red"
mosquitto_config_folder: "{{ config_root }}/mosquitto/config"
mosquitto_data_folder: "{{ config_root }}/mosquitto/data"
matter_server_data_folder: "{{ config_root }}/matter/data"
zigbee2mqtt_data_folder: "{{ config_root }}/z2m"
pihole_config_folder: "{{ config_root }}/pihole"
dnsmasq_config_folder: "{{ config_root }}/dnsmasq"
caddy_folder: "{{ config_root }}/caddy"
caddy_data_folder: "{{ config_root }}/caddy/data"
caddy_config_folder: "{{ config_root }}/caddy/config"
uptime_kuma_data_folder: "{{ config_root }}/uptime-kuma/data"
jellyfin_config_folder: "{{ config_root }}/jellyfin"
jellyfin_movie_folder: /tank/medien/Filme
jellyfin_shows_folder: /tank/medien/Shows
jellyfin_music_folder: /tank/medien/Musik
navidrome_data_folder: "{{ config_root }}/navidrome"
teslamate_data_folder: "{{ config_root }}/teslamate"
teslamate_grafana_data_folder: "{{ config_root }}/teslamate-grafana"
nextcloud_data_folder: "{{ config_root }}/nextcloud"
syncthing_config_folder: "{{ config_root }}/syncthing/config"
syncthing_data_folder: "{{ config_root }}/syncthing/data"
openwakeword_data_folder: "{{ config_root }}/openwakeword"
satellite_data_folder: "{{ config_root }}/satellite"
piper_data_folder: "{{ config_root }}/piper"
whisper_data_folder: "{{ config_root }}/whisper"
babybuddy_config_folder: "{{ config_root }}/babybuddy"
wallabag_config_folder: "{{ config_root }}/wallabag"
wallabag_images_folder: "{{ wallabag_config_folder }}/images"
wallabag_db_data_folder: "{{ wallabag_config_folder }}/data"
wallabag_domain_name: wallabag.lan
readeck_data_folder: "{{ config_root }}/readeck"
readeck_domain_name: readeck.lan
linkding_data_folder: "{{ config_root }}/linkding"
linkding_domain_name: linkding.lan
n8n_config_folder: "{{ config_root }}/n8n"
teddycloud_config_folder: "{{ config_root }}/teddycloud"
timemachine_max_size: 1000G
google_workspace_mcp_port_host: 8014
