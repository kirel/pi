ollama_models:
  - alias: Qwen3-4B-Instruct-2507-Q8
    name: qwen3:4b-instruct-2507-q8_0
    num_ctx: 16384

model_group_alias:
  home: "GPT-OSS-20B-vLLM-high"
  home-fast: "GPT-OSS-20B-vLLM-low"
  home-smart: "GPT-OSS-20B-vLLM-high"
  home-vision: "Qwen3-VL-8B-Instruct"
  home-tiny: "Qwen3-4B-Instruct-2507-Q8"

litellm_fallbacks:
  - home: ["home-tiny", "home-remote"]
  - home-fast: ["home-tiny", "home-remote"]
  - home-smart: ["home-remote"]

llamaswap_models:
  - alias: Qwen3-VL-8B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:Q8_K_XL
      --port ${PORT}
      --n-predict 16384
      --n-gpu-layers 99
      --cache-type-k q4_0
      --cache-type-v q4_0
      --top-p 0.8
      --top-k 20
      --temp 0.7
      --min-p 0.0
      --presence-penalty 1.5
      --jinja
      --fit on
    supports_function_calling: true
    supports_vision: true
    supports_reasoning: false
    group: "3090"
    env:
      - "CUDA_VISIBLE_DEVICES={{ampere_gpu_uuid}}"

  - alias: Qwen3-VL-32B-Instruct-Q6
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-VL-32B-Instruct-GGUF:Q6_K_XL
      --port ${PORT}
      --n-predict 16384
      --flash-attn auto
      --n-gpu-layers 99
      --ubatch-size 512
      --cache-type-k q4_0
      --cache-type-v q4_0
      --threads -1
      --top-p 0.8
      --top-k 20
      --temp 0.7
      --min-p 0.0
      --presence-penalty 1.5
      --jinja
      --fit on
      --fit-target 512,5120
      --swa-checkpoints 0
    supports_function_calling: true
    supports_vision: true
    supports_reasoning: false
    group: "big_boys"

  - alias: Qwen3-VL-32B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-VL-32B-Instruct-GGUF:Q4_K_XL
      --port ${PORT}
      --n-predict 16384
      --n-gpu-layers 99
      --cache-type-k q4_0
      --cache-type-v q4_0
      --top-p 0.8
      --top-k 20
      --temp 0.7
      --min-p 0.0
      --presence-penalty 1.5
      --jinja
      --fit on
      --fit-target 2048
    supports_function_calling: true
    supports_vision: true
    supports_reasoning: false
    group: "3090"
    env:
      - "CUDA_VISIBLE_DEVICES={{ampere_gpu_uuid}}"

  - alias: Qwen3-VL-30B-A3B-Thinking
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF:Q4_K_XL
      --port ${PORT}
      --n-predict 40960
      --n-gpu-layers 99
      --cache-type-k q4_0
      --cache-type-v q4_0
      --top-p 0.95
      --top-k 20
      --temp 1.0
      --min-p 0.0
      --presence-penalty 0.0
      --jinja
      --fit on
    supports_function_calling: true
    supports_vision: true
    supports_reasoning: true
    group: "3090"
    env:
      - "CUDA_VISIBLE_DEVICES={{ampere_gpu_uuid}}"

  - alias: Qwen3-Coder-30B-A3B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q4_K_XL
      --port ${PORT}
      --n-predict 32768
      --n-gpu-layers 99
      --cache-type-k q4_0
      --cache-type-v q4_0
      --temp 0.7
      --min-p 0.0
      --top-p 0.80
      --top-k 20
      --repeat-penalty 1.05
      --jinja
      --fit on
    supports_function_calling: true
    supports_vision: false
    supports_reasoning: false
    group: "3090"
    env:
      - "CUDA_VISIBLE_DEVICES={{ampere_gpu_uuid}}"

  - alias: Nemotron-3-Nano-30B-A3B
    cmd: >-
      /app/llama-server
      -hf unsloth/Nemotron-3-Nano-30B-A3B-GGUF:Q4_K_XL
      --port ${PORT}
      --n-predict 16384
      --n-gpu-layers 99
      --prio 3
      --min-p 0.01
      --temp 0.6
      --top-p 0.95
      --jinja
      --fit on
    supports_function_calling: true
    supports_vision: false
    supports_reasoning: true
    group: "3090"
    env:
      - "CUDA_VISIBLE_DEVICES={{ampere_gpu_uuid}}"
    variants:
      nothink:
        chat_template_kwargs:
          enable_thinking: false

  - alias: Nemotron-3-Nano-30B-A3B-Q6
    cmd: >-
      /app/llama-server
      -hf unsloth/Nemotron-3-Nano-30B-A3B-GGUF:Q6_K_XL
      --port ${PORT}
      --n-predict 16384
      --flash-attn auto
      --n-gpu-layers 99
      --ubatch-size 512
      --cache-type-k f16
      --cache-type-v f16
      --threads -1
      --prio 3
      --min-p 0.01
      --temp 0.6
      --top-p 0.95
      --jinja
      --fit on
      --fit-target 512,5120
      --swa-checkpoints 0
    supports_function_calling: true
    supports_vision: false
    supports_reasoning: true
    group: "big_boys"
    variants:
      nothink:
        chat_template_kwargs:
          enable_thinking: false

  - alias: Devstral-Small-2-24B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF:Q5_K_XL
      --port ${PORT}
      --n-predict 32768
      --n-gpu-layers 99
      --cache-type-k q4_0
      --cache-type-v q4_0
      --temp 0.15
      --min-p 0.01
      --jinja
      --fit on
    supports_function_calling: true
    supports_vision: true
    supports_reasoning: false
    group: "3090"
    env:
      - "CUDA_VISIBLE_DEVICES={{ampere_gpu_uuid}}"

  - alias: GLM-4.6V-Flash
    cmd: >-
      /app/llama-server
      -hf unsloth/GLM-4.6V-Flash-GGUF:Q8_K_XL
      --port ${PORT}
      --n-gpu-layers 99
      --cache-type-k q4_0
      --cache-type-v q4_0
      --top-p 0.6
      --top-k 2
      --temp 0.8
      --repeat-penalty 1.1
      --n-predict 16384
      --jinja
      --fit on
    supports_function_calling: true
    supports_vision: true
    supports_reasoning: true
    group: "3090"
    env:
      - "CUDA_VISIBLE_DEVICES={{ampere_gpu_uuid}}"
    variants:
      nothink:
        chat_template_kwargs:
          enable_thinking: false

  - alias: Qwen3-Next-80B-A3B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF:Q4_K_XL
      --port ${PORT}
      --flash-attn auto
      --n-gpu-layers 99
      --ubatch-size 512
      --cache-type-k q4_0
      --cache-type-v q4_0
      --threads -1
      --temp 0.7
      --top-p 0.8
      --top-k 20
      --min-p 0.0
      --presence-penalty 1.0
      --n-predict 16384
      --n-cpu-moe 15
      --jinja
      --fit on
      --fit-target 512,5120
      --swa-checkpoints 0
    supports_function_calling: true
    supports_vision: false
    supports_reasoning: false
    group: "big_boys"

  - alias: Seed-OSS-36B-Instruct-Magic
    cmd: >-
      /app/llama-server
      -hf magiccodingman/Seed-OSS-36B-Instruct-unsloth-MagicQuant-Hybrid-GGUF:Seed-OSS-36B-Instruct-mxfp4_moe-EHQKOUD-IQ4NL.gguf
      --port ${PORT}
      --n-gpu-layers 99
      --cache-type-k q4_0
      --cache-type-v q4_0
      --temp 1.1
      --top-p 0.95
      --n-predict 16384
      --jinja
      --fit on
    supports_function_calling: true
    supports_vision: false
    supports_reasoning: true
    group: "3090"
    env:
      - "CUDA_VISIBLE_DEVICES={{ampere_gpu_uuid}}"
    variants:
      high:
        chat_template_kwargs:
          thinking_budget: 2048
      medium:
        chat_template_kwargs:
          thinking_budget: 1024
      low:
        chat_template_kwargs:
          thinking_budget: 512
      nothink:
        chat_template_kwargs:
          thinking_budget: 0

  - alias: Seed-OSS-36B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Seed-OSS-36B-Instruct-GGUF:Q5_K_XL
      --port ${PORT}
      --n-predict 16384
      --flash-attn auto
      --n-gpu-layers 99
      --ubatch-size 512
      --cache-type-k q4_0
      --cache-type-v q4_0
      --threads -1
      --temp 1.1
      --top-p 0.95
      --jinja
      --fit on
      --fit-target 512,5120
      --swa-checkpoints 0
    supports_function_calling: true
    supports_vision: false
    supports_reasoning: true
    group: "big_boys"
    variants:
      high:
        chat_template_kwargs:
          thinking_budget: 2048
      medium:
        chat_template_kwargs:
          thinking_budget: 1024
      low:
        chat_template_kwargs:
          thinking_budget: 512
      nothink:
        chat_template_kwargs:
          thinking_budget: 0

  - alias: gpt-oss-120b
    cmd: >-
      /app/llama-server
      -hf unsloth/gpt-oss-120b-GGUF:F16
      --port ${PORT}
      --threads 12
      --threads-batch 12
      --ctx-size 16384
      --n-predict 16384
      --n-gpu-layers 99
      --cache-type-k q4_0
      --cache-type-v q4_0
      --temp 1.0
      --min-p 0.0
      --top-p 1.0
      --top-k 0.0
      -ot ".*attn_.*=CUDA0"
      -ot ".*ffn_(norm|gate_inp).*=CUDA0"
      -ot "blk\\.([0-9]|1[0-9]|2[0-4])\\.ffn_.*_exps.*=CPU"
      -ot "blk\\.(2[5-9]|3[0-5])\\.ffn_.*_exps.*=CUDA0"
      --jinja
    supports_function_calling: true
    supports_vision: false
    supports_reasoning: true
    group: "3090"
    env:
      - "CUDA_VISIBLE_DEVICES={{ampere_gpu_uuid}}"
    variants:
      high:
        chat_template_kwargs:
          reasoning_effort: "high"
      low:
        chat_template_kwargs:
          reasoning_effort: "low"

  - alias: gpt-oss-120b-hybrid
    cmd: >-
      /app/llama-server
      -hf unsloth/gpt-oss-120b-GGUF:F16
      --port ${PORT}
      --threads 12
      --threads-batch 12
      --ctx-size 16384
      --n-predict 16384
      --n-gpu-layers 99
      --cache-type-k q4_0
      --cache-type-v q4_0
      --temp 1.0
      --min-p 0.0
      --top-p 1.0
      --top-k 0.0
      -ot ".*attn_.*=CUDA0"
      -ot ".*ffn_(norm|gate_inp).*=CUDA0"
      -ot "blk\\.([0-4])\\.ffn_.*_exps.*=CUDA1"
      -ot "blk\\.([5-9]|1[0-9]|2[0-4])\\.ffn_.*_exps.*=CPU"
      -ot "blk\\.(2[5-9]|3[0-5])\\.ffn_.*_exps.*=CUDA0"
      --ubatch-size 512
      --jinja
      --swa-checkpoints 0
    supports_function_calling: true
    supports_vision: false
    supports_reasoning: true
    group: "big_boys"
    variants:
      high:
        chat_template_kwargs:
          reasoning_effort: "high"
      low:
        chat_template_kwargs:
          reasoning_effort: "low"

  - alias: GPT-OSS-20B-F16
    cmd: >-
      /app/llama-server
      -hf unsloth/gpt-oss-20b-GGUF:F16
      --port ${PORT}
      --n-predict 16384
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --temp 1.0
      --top-p 1.0
      --top-k 0
      --jinja
      --fit on
    supports_function_calling: true
    supports_vision: false
    group: "3090"
    env:
      - "CUDA_VISIBLE_DEVICES={{ampere_gpu_uuid}}"
    variants:
      high:
        chat_template_kwargs:
          reasoning_effort: "high"
      low:
        chat_template_kwargs:
          reasoning_effort: "low"

  - alias: GPT-OSS-20B-blackwell
    cmd: >-
      /app/llama-server
      -hf unsloth/gpt-oss-20b-GGUF:F16
      --port ${PORT}
      --n-predict 16384
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --threads -1
      --temp 1.0
      --top-p 1.0
      --top-k 0
      --jinja
      --fit on
      --fit-target 5120
    supports_function_calling: true
    supports_vision: false
    group: "5060ti"
    env:
      - "CUDA_VISIBLE_DEVICES={{blackwell_gpu_uuid}}"
    variants:
      high:
        chat_template_kwargs:
          reasoning_effort: "high"
      low:
        chat_template_kwargs:
          reasoning_effort: "low"

  - alias: GPT-OSS-20B-vLLM
    cmd: >-
      docker run --rm --init --name ${MODEL_ID}
      --runtime nvidia --gpus '"device={{ampere_gpu_uuid}}"'
      --network container:llama-swap
      -v {{ llamaswap_config_path }}/cache:/root/.cache
      --ipc=host
      -e HF_HOME=/root/.cache/huggingface
      vllm/vllm-openai:v0.13.0
      openai/gpt-oss-20b
      --served-model-name ${MODEL_ID}
      --port ${PORT}
      --max-num-seqs 1
      --max-num-batched-tokens 2048
      --stream-interval 1
      --tool-call-parser openai
      --enable-auto-tool-choice
    cmdStop: docker stop ${MODEL_ID}
    supports_function_calling: true
    supports_vision: false
    group: "3090"
    variants:
      high:
        extra_body:
          reasoning_effort: "high"
      low:
        extra_body:
          reasoning_effort: "low"

  - alias: GLM-4.7-Flash-AWQ-vLLM
    cmd: >-
      docker run --rm --init --name ${MODEL_ID}
      --runtime nvidia --gpus '"device={{ampere_gpu_uuid}}"'
      --network container:llama-swap
      -v {{ llamaswap_config_path }}/cache:/root/.cache
      --ipc=host
      -e HF_HOME=/root/.cache/huggingface
      vllm/vllm-openai:nightly
      cyankiwi/GLM-4.7-Flash-AWQ-4bit
      --served-model-name ${MODEL_ID}
      --port ${PORT}
      --max-num-seqs 1
      --max-num-batched-tokens 2048
      --stream-interval 1
      --speculative-config.method mtp
      --speculative-config.num_speculative_tokens 1
      --tool-call-parser glm47
      --reasoning-parser glm45
      --enable-auto-tool-choice
    cmdStop: docker stop ${MODEL_ID}
    supports_function_calling: true
    supports_vision: false
    supports_reasoning: true
    group: "3090"

llamaswap_model_groups:
  forever:
    persistent: true
    swap: false
    exclusive: false
  "3090":
    swap: true
    exclusive: false
  "5060ti":
    swap: true
    exclusive: false
  big_boys:
    swap: true
    exclusive: true

llamaswap_preload:
  - "{{ model_group_alias.home_tiny }}"
  - "{{ model_group_alias.home_fast }}"
