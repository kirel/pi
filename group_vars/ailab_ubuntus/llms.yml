model_group_alias:
  home: "GPT-OSS-20B-F16"
  home-fast: "GPT-OSS-20B-F16-low"
  home-smart: "GPT-OSS-20B-F16-high"
  home-vision: "Ministral-3-14B-Reasoning-2512"
  home-tiny: "Qwen3-VL-32B-Instruct"
llamaswap_models:
  - alias: Qwen3-VL-8B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:Q8_K_XL
      --port ${PORT}
      --ctx-size 32768
      --flash-attn auto
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --threads -1
      --top-p 0.8
      --top-k 20
      --temp 0.7
      --min-p 0.0
      --presence-penalty 1.5
      --jinja
    supports_function_calling: true
    supports_vision: true
    group: swap
  - alias: Qwen3-VL-32B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-VL-32B-Instruct-GGUF:Q6_K_XL
      --port ${PORT}
      --ctx-size 22768
      --flash-attn auto
      --n-gpu-layers 99
      --tensor-split 40,16
      --cache-type-k q8_0
      --cache-type-v q8_0
      --threads -1
      --top-p 0.8
      --top-k 20
      --temp 0.7
      --min-p 0.0
      --presence-penalty 1.5
      --jinja
    supports_function_calling: true
    supports_vision: true
    group: swap
  - alias: Qwen3-VL-30B-A3B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF:Q4_K_XL
      --port ${PORT}
      --ctx-size 32768
      --n-predict 32768
      --flash-attn auto
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --threads -1
      --batch-size 512
      --ubatch-size 512
      --n-cpu-moe 0
      --top-p 0.8
      --top-k 20
      --temp 0.7
      --min-p 0.0
      --presence-penalty 1.5
      --jinja
    supports_function_calling: true
    supports_vision: true
    group: swap
  - alias: Qwen3-VL-30B-A3B-Thinking
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF:Q4_K_XL
      --port ${PORT}
      --ctx-size 32768
      --n-predict 40960
      --flash-attn auto
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --n-cpu-moe 5
      --top-p 0.95
      --top-k 20
      --temp 1.0
      --min-p 0.0
      --presence-penalty 0.0
      --jinja
    supports_function_calling: true
    supports_vision: true
    group: swap
  - alias: Qwen3-Coder-30B-A3B-Instruct
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q4_K_XL
      --port ${PORT}
      --ctx-size 32768
      --n-predict 32768
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --n-cpu-moe 20
      --threads -1
      --temp 0.7
      --min-p 0.0
      --top-p 0.80
      --top-k 20
      --repeat-penalty 1.05
      --jinja
    supports_function_calling: true
    supports_vision: false
    group: swap
  - alias: Qwen3-30B-A3B-Instruct-2507
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q4_K_XL
      --port ${PORT}
      --ctx-size 32768
      --n-predict 32768
      --flash-attn auto
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --threads -1
      --batch-size 512
      --ubatch-size 512
      --n-cpu-moe 0
      --temp 0.7
      --min-p 0.0
      --top-p 0.8
      --top-k 20
      --presence-penalty 1.0
      --jinja
    supports_function_calling: true
    supports_vision: false
    group: swap
  - alias: GPT-OSS-20B-F16
    cmd: >-
      /app/llama-server
      -hf unsloth/gpt-oss-20b-GGUF:F16
      --port ${PORT}
      --ctx-size 131072
      --n-gpu-layers 99
      --cache-type-k q8_0
      --cache-type-v q8_0
      --threads -1
      --temp 1.0
      --top-p 1.0
      --top-k 0
      --jinja
    supports_function_calling: true
    supports_vision: false
    group: swap
    env:
      - "CUDA_VISIBLE_DEVICES=0"
    variants:
      high:
        chat_template_kwargs:
          reasoning_effort: "high"
      low:
        chat_template_kwargs:
          reasoning_effort: "low"
  - alias: Ministral-3-14B-Instruct-2512
    cmd: >-
      /app/llama-server
      -hf unsloth/Ministral-3-14B-Instruct-2512-GGUF:Q6_K_XL
      --port ${PORT}
      --ctx-size 32768
      --flash-attn auto
      --n-gpu-layers 99
      --threads -1
      --cache-type-k q8_0
      --cache-type-v q8_0
      --temp 0.15
      --jinja
    supports_function_calling: true
    supports_vision: true
    group: swap
  - alias: Ministral-3-14B-Reasoning-2512
    cmd: >-
      /app/llama-server
      -hf unsloth/Ministral-3-14B-Reasoning-2512-GGUF:Q6_K_XL
      --port ${PORT}
      --ctx-size 32768
      --flash-attn auto
      --n-gpu-layers 99
      --threads -1
      --top-p 0.95
      --cache-type-k q8_0
      --cache-type-v q8_0
      --temp 0.7
      --jinja
    supports_function_calling: true
    supports_vision: true
    group: swap
  - alias: Qwen3-0.6B-Q4
    cmd: >-
      /app/llama-server
      -hf unsloth/Qwen3-0.6B-GGUF:Q6_K_XL
      --port ${PORT}
      --ctx-size 4096
      --flash-attn auto
      --n-gpu-layers 0
      --cache-type-k q8_0
      --cache-type-v q8_0
      --temp 0.7
      --top-p 0.8
      --top-k 20
      --min-p 0.0
      --chat-template-kwargs '{"enable_thinking": false}'
      --jinja
    env:
      - "CUDA_VISIBLE_DEVICES="
    supports_function_calling: true
    supports_vision: false
    group: forever

llamaswap_model_groups:
  forever:
    persistent: true
    swap: false
    exclusive: false
  swap:
    swap: true

llamaswap_preload:
  - "{{ model_group_alias.home_tiny }}"
  - "{{ model_group_alias.home_fast }}"
