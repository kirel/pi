- name: Create volume
  community.docker.docker_volume:
    name: ollama_data

- name: Start AI containers
  community.docker.docker_compose_v2:
    project_name: ai
    pull: "always"  # Options: "always", "missing", "never", or "policy"
    state: present  # Ensure the services are started
    definition:
      services:
        ollama-service:
          image: ollama/ollama
          container_name: ollama
          restart: unless-stopped
          ports:
            - "11435:11434"
          environment:
            OLLAMA_DEBUG: "1"
            OLLAMA_NUM_PARALLEL: "2"
            OLLAMA_MAX_LOADED_MODELS: "2"
          volumes:
            - ollama_data:/root/.ollama
          labels:
            wud.tag.include: '^\d+\.\d+\.\d+$$'

        open-webui:
          image: ghcr.io/open-webui/open-webui:main
          container_name: open-webui
          restart: unless-stopped
          environment:
            OLLAMA_BASE_URL: "http://ollama-raw.lan:11434"
          volumes:
            - open-webui:/app/backend/data
          ports:
            - "{{ open_webui_http_port }}:8080"
          labels:
            wud.tag.include: 'main'
            wud.watch.digest: 'true'
            homepage.group: "{{ services['open-webui'].group }}"
            homepage.name: "{{ services['open-webui'].name }}"
            homepage.href: "https://ollama.lan"
            homepage.icon: openai.png
            homepage.server: "{{ inventory_hostname }}"
            homepage.container: open-webui

      volumes:
        ollama_data:
          external: true  # External volume for persistent data
        open-webui: {}  # Internal volume for web UI data
