model_list:
  - model_name: "{{ llama_server_model_configs[selected_llama_server_model_key].alias }}"
    litellm_params:
      model: "openai/{{ llama_server_model_configs[selected_llama_server_model_key].alias }}" # This is the model name LiteLLM will request from the backend
      api_base: "http://{{ ollama_arc_container_name }}:8008/v1" # Points to the OpenAI-compatible /v1 endpoint of your llama-server
      api_key: none

litellm_settings:
  success_callback: ["langfuse"]

general_settings:
  master_key: "{{ litellm_master_key }}"
