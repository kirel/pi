---
# defaults file for ollama-arc
ollama_arc_image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
ipex_vllm_image: intelanalytics/ipex-llm-serving-xpu:latest
ollama_arc_container_name: ipex-llm-inference-cpp-xpu-container
ipex_vllm_container_name: ipex-llm-serving-xpu-container
ollama_arc_models_path: /opt/ollama-models # Default path on host, override in group_vars if needed
ollama_arc_memory: 32G
ollama_arc_shm_size: 16g
ollama_arc_device_type: Arc # Or Flex, Max, iGPU
ollama_arc_benchmark_model: "" # Set to a model filename like "mistral-7b-v0.1.Q4_0.gguf" to enable benchmark env var
ollama_arc_no_proxy: localhost,127.0.0.1
ollama_arc_restart_policy: always

# Build options
ollama_arc_build_repo_url: https://github.com/intel/ipex-llm.git
ollama_arc_build_repo_version: main # Or specific tag/commit
ollama_arc_build_repo_dest: /opt/src/ipex-llm # Path on target host to clone the repo
ollama_arc_build_context_subdir: docker/llm/inference-cpp # Subdirectory within the repo containing the Dockerfile
ipex_vllm_build_context_subdir: docker/llm/serving/xpu/docker
ollama_arc_build_nocache: false # Corresponds to --no-cache, usually false for automation
ollama_arc_build_rm: true # Corresponds to --rm
ollama_arc_build_args: # Corresponds to --build-arg
  http_proxy: "" # Set via group_vars/host_vars if needed
  https_proxy: "" # Set via group_vars/host_vars if needed
  no_proxy: "{{ ollama_arc_no_proxy }}" # Default to the container's no_proxy setting

vllm_repo: https://github.com/vllm-project/vllm.git
vllm_repo_dest: /opt/src/vllm
vllm_image: vllm-xpu:latest
vllm_repo_version: main
vllm_build_context_subdir: docker
vllm_build_dockerfile: Dockerfile.xpu
vllm_models_path: /opt/vllm-models # Default path on host, override in group_vars if needed
vllm_container_name: vllm-xpu
