---
# defaults file for ollama-arc
ollama_arc_image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
ipex_vllm_image: intelanalytics/ipex-llm-serving-xpu:latest
ollama_arc_container_name: ipex-llm-inference-cpp-xpu-container
ipex_vllm_container_name: ipex-llm-serving-xpu-container
ollama_arc_models_path: /opt/ollama-models # Default path on host, override in group_vars if needed
ollama_arc_memory: 32G
ollama_arc_shm_size: 16g
ollama_arc_device_type: Arc # Or Flex, Max, iGPU
ollama_arc_benchmark_model: "" # Set to a model filename like "mistral-7b-v0.1.Q4_0.gguf" to enable benchmark env var
ollama_arc_no_proxy: localhost,127.0.0.1
ollama_arc_restart_policy: always

# Build options
ollama_arc_build_repo_url: https://github.com/intel/ipex-llm.git
ollama_arc_build_repo_version: main # Or specific tag/commit
ollama_arc_build_repo_dest: /opt/src/ipex-llm # Path on target host to clone the repo
ollama_arc_build_context_subdir: docker/llm/inference-cpp # Subdirectory within the repo containing the Dockerfile
ollama_arc_build_nocache: true # Corresponds to --no-cache, usually false for automation
ollama_arc_build_rm: false # Corresponds to --rm
ollama_arc_build_args: # Corresponds to --build-arg
  http_proxy: "" # Set via group_vars/host_vars if needed
  https_proxy: "" # Set via group_vars/host_vars if needed
  no_proxy: "{{ ollama_arc_no_proxy }}" # Default to the container's no_proxy setting

ipex_vllm_build_context_subdir: docker/llm/serving/xpu/docker
ipex_vllm_script_host_path: /opt/vllm

vllm_models_path: /opt/vllm/models # Default path on host, override in group_vars if needed

vllm_repo: https://github.com/vllm-project/vllm.git
vllm_repo_dest: /opt/vllm/src
vllm_image: vllm-xpu:latest
vllm_repo_version: main
vllm_build_context_subdir: docker
vllm_build_dockerfile: Dockerfile.xpu
vllm_container_name: vllm-xpu

# --- Portable Ollama Build Settings ---
# URL for the Ollama portable package (Update this URL if a newer/stable version is available)
ollama_arc_portable_download_url: "https://github.com/ipex-llm/ipex-llm/releases/download/v2.3.0-nightly/ipex-llm-ollama-linux-portable-gpu-2.3.0-nightly.tgz"

# Image name for the portable version
ollama_arc_image_portable: "ollama-portable-xpu:latest"
# Remote path for portable build context
ollama_arc_portable_build_dest: "/opt/build/ollama-portable"
