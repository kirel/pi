---
# defaults file for ollama-arc
ollama_arc_image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
ipex_vllm_image: intelanalytics/ipex-llm-serving-xpu:latest
ollama_arc_container_name: ipex-llm-inference-cpp-xpu-container
ipex_vllm_container_name: ipex-llm-serving-xpu-container
ollama_arc_models_path: /opt/ollama-models # Default path on host, override in group_vars if needed
ollama_arc_memory: 32G
ollama_arc_shm_size: 16g
ollama_arc_device_type: Arc # Or Flex, Max, iGPU
ollama_arc_benchmark_model: "" # Set to a model filename like "mistral-7b-v0.1.Q4_0.gguf" to enable benchmark env var
ollama_arc_no_proxy: localhost,127.0.0.1
ollama_arc_restart_policy: always

# Build options
ollama_arc_base_src_path: /opt/src
ollama_arc_base_config_path: /opt/config # Base path for configuration files
ollama_arc_build_repo_url: https://github.com/intel/ipex-llm.git
ollama_arc_build_repo_version: main # Or specific tag/commit
ollama_arc_build_repo_dest: "{{ ollama_arc_base_src_path }}/ipex-llm" # Path on target host to clone the repo
ollama_arc_build_context_subdir: docker/llm/inference-cpp # Subdirectory within the repo containing the Dockerfile
ollama_arc_build_nocache: true # Corresponds to --no-cache, usually false for automation
ollama_arc_build_rm: false # Corresponds to --rm
ollama_arc_build_args: # Corresponds to --build-arg
  http_proxy: "" # Set via group_vars/host_vars if needed
  https_proxy: "" # Set via group_vars/host_vars if needed
  no_proxy: "{{ ollama_arc_no_proxy }}" # Default to the container's no_proxy setting

ipex_vllm_build_context_subdir: docker/llm/serving/xpu/docker
ipex_vllm_script_host_path: /opt/vllm

vllm_models_path: /opt/vllm/models # Default path on host, override in group_vars if needed

vllm_repo: https://github.com/vllm-project/vllm.git
vllm_repo_dest: /opt/vllm/src
vllm_image: vllm-xpu:latest
vllm_repo_version: main
vllm_build_context_subdir: docker
vllm_build_dockerfile: Dockerfile.xpu
vllm_container_name: vllm-xpu

# --- Portable Ollama Build Settings ---
# URL for the Ollama portable package (Update this URL if a newer/stable version is available)
ollama_arc_portable_download_url: "https://github.com/ipex-llm/ipex-llm/releases/download/v2.3.0-nightly/ipex-llm-ollama-linux-portable-gpu-2.3.0-nightly.tgz"

# Image name for the portable version
ollama_arc_image_portable: "ollama-portable-xpu:latest"
# Remote path for portable build context
ollama_arc_portable_build_dest: "/opt/build/ollama-portable"

# --- MCP-Bridge Settings ---
mcp_bridge_repo_url: "https://github.com/SecretiveShell/MCP-Bridge.git"
mcp_bridge_repo_version: "master" # Or a specific tag/commit
mcp_bridge_src_path: "{{ ollama_arc_base_src_path }}/mcp-bridge-src" # Path on target host to clone the repo
mcp_bridge_image: "mcp-bridge:ansible-built"
mcp_bridge_container_name: "mcp-bridge-container"
mcp_bridge_service_port: 8008 # Port MCP-Bridge will listen on (on the host network)
mcp_bridge_target_host: ailab.lan # Port of the vLLM service (usually localhost as both use host network)
mcp_bridge_target_port: 8080 # Port of the vLLM service (usually localhost as both use host network)
mcp_bridge_config_dir_host: "/opt/config/mcp-bridge" # Directory on host to store config.json
mcp_bridge_config_filename: "config.json"

# --- Llama Server Configuration ---

# Gemeinsame Kommandozeilenargumente für llama-server
# Umgebungsvariablen wie $OLLAMA_NUM_CTX werden im Container-Kontext aufgelöst.
llama_server_common_args: >-
  --jinja -ngl 999 -t 1 -b 1024 -b 1024 -ub 512
  --metrics --slots --no-mmap --mlock
  -c $OLLAMA_NUM_CTX --host 0.0.0.0

ollama_arc_qwen3_8b_q4_gguf_path: "/models/blobs/sha256-34a514d08f7449cb4a694a707aaa2eedccb7bb68290121bf5e5a569b2abe71c3"
ollama_arc_qwen3_8b_q6_gguf_path: "/models/blobs/sha256-61201e42b1c1021ce15936f23682e56e873bb4af3e8e5aae7558bc1c53a1dd91"
ollama_arc_qwen3_14b_gguf_path: "/models/blobs/sha256-8cf51b9cafe0b107a61df6e3b230cb0cf7c47414e1e47340f626466dfad4e9e8"
ollama_arc_qwen3_30b_gguf_path: "/models/blobs/sha256-b99d9c80fb8278c79c355dc77a5299d73420022f332bc4d747c197207c24fc7c"
ollama_arc_qwen3_32b_gguf_path: "/models/blobs/sha256-8df67573b2c23484e02ec7af295e39bed7ee774f3771d5fda2978265b59370e7"

llama_server_qwen3_generation_args: >-
  --top-k 20 --top-p 0.95 --min-p 0 --presence-penalty 1.5 -n 32768 --no-context-shift --reasoning-format deepseek --jinja

# Modellspezifische Konfigurationen für llama-server
# Die model_env_var verweist auf Umgebungsvariablen, die im Container gesetzt sind (siehe ollama_arc_env_vars in tasks/main.yml)
llama_server_model_configs:
  qwen3_8b_q4:
    model_env_var: "{{ ollama_arc_qwen3_8b_q4_gguf_path }}"
    alias: "Qwen3-8B"
    extra_args: >-
      {{ llama_server_qwen3_generation_args }}
      -mg 1 -ts 16,1
  qwen3_8b_q6:
    model_env_var: "{{ ollama_arc_qwen3_8b_q4_gguf_path }}"
    alias: "Qwen3-8B"
    extra_args: >-
      {{ llama_server_qwen3_generation_args }}
  qwen3_14b:
    model_env_var: "{{ ollama_arc_qwen3_14b_gguf_path }}"
    alias: "Qwen3-14B"
    extra_args: >-
      {{ llama_server_qwen3_generation_args }}
  qwen3_32b:
    model_env_var: "{{ ollama_arc_qwen3_32b_gguf_path }}"
    alias: "Qwen3-32B"
    extra_args: >-
      {{ llama_server_qwen3_generation_args }}
      -mg 0 -ts 16,16,12 --override-tensor "token\_embd.weight=SYCL0"
  qwen3_30b:
    model_env_var: "{{ ollama_arc_qwen3_30b_gguf_path }}"
    alias: "Qwen3-30B"
    extra_args: >-
      {{ llama_server_qwen3_generation_args }}
  qwen3_30b_cpu_offload:
    model_env_var: "{{ ollama_arc_qwen3_30b_gguf_path }}"
    alias: "Qwen3-30B"
    extra_args: >-
      -mg 0 -ts 1,1 -ot 'blk\.[0-3]\.(ffn_gate|ffn_up|ffn_down)\.weight=CPU'
      --ubatch-size 4096
      {{ llama_server_qwen3_generation_args }}

# Standardmäßig ausgewähltes Modell für den llama-server
# Dies kann in group_vars/host_vars oder über --extra-vars überschrieben werden.
selected_llama_server_model_key: "qwen3_14b"

# --- LiteLLM Proxy Settings ---
litellm_proxy_image: "ghcr.io/berriai/litellm:main-latest"
litellm_proxy_container_name: "litellm-proxy-container"
litellm_proxy_config_dir_host: "{{ ollama_arc_base_config_path }}/litellm"
litellm_proxy_config_filename: "config.yaml"
litellm_proxy_port_host: "4000" # Host port for LiteLLM
litellm_proxy_port_container: "4000" # Container port for LiteLLM (default)
langfuse_host_for_litellm: "http://ailab.lan:3000"
