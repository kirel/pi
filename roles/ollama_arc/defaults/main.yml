---
# defaults file for ollama-arc
ollama_arc_image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
ipex_vllm_image: intelanalytics/ipex-llm-serving-xpu:latest
ollama_arc_container_name: ipex-llm-inference-cpp-xpu-container
ipex_vllm_container_name: ipex-llm-serving-xpu-container
ollama_arc_models_path: /opt/ollama-models # Default path on host, override in group_vars if needed
ollama_arc_memory: 32G
ollama_arc_shm_size: 16g
ollama_arc_device_type: Arc # Or Flex, Max, iGPU
ollama_arc_benchmark_model: "" # Set to a model filename like "mistral-7b-v0.1.Q4_0.gguf" to enable benchmark env var
ollama_arc_no_proxy: localhost,127.0.0.1
ollama_arc_restart_policy: always

# Build options
ollama_arc_base_src_path: /opt/src
ollama_arc_build_repo_url: https://github.com/intel/ipex-llm.git
ollama_arc_build_repo_version: main # Or specific tag/commit
ollama_arc_build_repo_dest: "{{ ollama_arc_base_src_path }}/ipex-llm" # Path on target host to clone the repo
ollama_arc_build_context_subdir: docker/llm/inference-cpp # Subdirectory within the repo containing the Dockerfile
ollama_arc_build_nocache: true # Corresponds to --no-cache, usually false for automation
ollama_arc_build_rm: false # Corresponds to --rm
ollama_arc_build_args: # Corresponds to --build-arg
  http_proxy: "" # Set via group_vars/host_vars if needed
  https_proxy: "" # Set via group_vars/host_vars if needed
  no_proxy: "{{ ollama_arc_no_proxy }}" # Default to the container's no_proxy setting

ipex_vllm_build_context_subdir: docker/llm/serving/xpu/docker
ipex_vllm_script_host_path: /opt/vllm

vllm_models_path: /opt/vllm/models # Default path on host, override in group_vars if needed

vllm_repo: https://github.com/vllm-project/vllm.git
vllm_repo_dest: /opt/vllm/src
vllm_image: vllm-xpu:latest
vllm_repo_version: main
vllm_build_context_subdir: docker
vllm_build_dockerfile: Dockerfile.xpu
vllm_container_name: vllm-xpu

# --- Portable Ollama Build Settings ---
# URL for the Ollama portable package (Update this URL if a newer/stable version is available)
ollama_arc_portable_download_url: "https://github.com/ipex-llm/ipex-llm/releases/download/v2.3.0-nightly/ipex-llm-ollama-linux-portable-gpu-2.3.0-nightly.tgz"

# Image name for the portable version
ollama_arc_image_portable: "ollama-portable-xpu:latest"
# Remote path for portable build context
ollama_arc_portable_build_dest: "/opt/build/ollama-portable"

# --- MCP-Bridge Settings ---
mcp_bridge_repo_url: "https://github.com/SecretiveShell/MCP-Bridge.git"
mcp_bridge_repo_version: "master" # Or a specific tag/commit
mcp_bridge_src_path: "{{ ollama_arc_base_src_path }}/mcp-bridge-src" # Path on target host to clone the repo
mcp_bridge_image: "mcp-bridge:ansible-built"
mcp_bridge_container_name: "mcp-bridge-container"
mcp_bridge_service_port: 8008 # Port MCP-Bridge will listen on (on the host network)
mcp_bridge_target_port: 8001 # Port of the vLLM service (usually localhost as both use host network)
mcp_bridge_config_dir_host: "/opt/config/mcp-bridge" # Directory on host to store config.json
mcp_bridge_config_filename: "config.json"

# --- Llama Server Configuration ---
# Gemeinsame Kommandozeilenargumente für llama-server
# Umgebungsvariablen wie $OLLAMA_NUM_CTX werden im Container-Kontext aufgelöst.
llama_server_common_args: >-
  --jinja -ngl 999 -t 11
  --metrics --slots --no-mmap --top-k 20 --top-p 0.95 --min-p 0 --presence-penalty 1.5
  -c $OLLAMA_NUM_CTX -n 32768 --no-context-shift --host 0.0.0.0

# Modellspezifische Konfigurationen für llama-server
# Die model_env_var verweist auf Umgebungsvariablen, die im Container gesetzt sind (siehe ollama_arc_env_vars in tasks/main.yml)
llama_server_model_configs:
  qwen3_30b_cpu_offload:
    model_env_var: "$QWEN3_30B_GGUF"
    alias: "Qwen3-30B"
    extra_args: >-
       -b 1024 -mg 0 -ts 1,1
        -ot 'blk\.(\d|1\d|2[0-5])\.ffn_.*_exps.=CPU'
  qwen3_14b:
    model_env_var: "$QWEN3_14B_GGUF"
    alias: "Qwen3-14B"
    extra_args: "" # Beispiel: "--some-specific-flag value"
  # Füge hier weitere Modellkonfigurationen hinzu
  # Beispiel für ein anderes Modell, falls du die entsprechende GGUF-Umgebungsvariable definierst:
  # qwen3_32b:
  #   model_env_var: "$QWEN3_32B_GGUF"
  #   alias: "Qwen3-32B"
  #   extra_args: ""

# Standardmäßig ausgewähltes Modell für den llama-server
# Dies kann in group_vars/host_vars oder über --extra-vars überschrieben werden.
selected_llama_server_model_key: "qwen3_30b"
