---
# tasks file for ollama-arc

- name: Ensure models directory exists
  ansible.builtin.file:
    path: "{{ ollama_arc_models_path }}"
    state: directory
    mode: "0755"
  become: true

# Block for building the image locally (always runs)
- name: Build ipex-llm-inference-cpp-xpu image from source
  become: true # Often needed for git checkout destination and docker build
  block:
    - name: Ensure git package is present for cloning
      ansible.builtin.package:
        name: git
        state: latest

    - name: Ensure build destination directory exists
      ansible.builtin.file:
        path: "{{ ollama_arc_build_repo_dest | ansible.builtin.dirname }}" # Ensure parent dir exists
        state: directory
        mode: "0755"

    - name: Clone or update the ipex-llm repository
      ansible.builtin.git:
        repo: "{{ ollama_arc_build_repo_url }}"
        dest: "{{ ollama_arc_build_repo_dest }}"
        version: "{{ ollama_arc_build_repo_version }}"
        force: true # Ensure the correct version is checked out
        depth: 1 # Perform a shallow clone
      register: git_clone_result # Register the result

    - name: Clone or update the vllm repository #
      ansible.builtin.git:
        repo: "{{ vllm_repo }}"
        dest: "{{ vllm_repo_dest }}"
        version : "{{ vllm_repo_version }}"
        force: true
        depth: 1
      register: vllm_clone_result

    - name: Build the ipex-llm-inference-cpp-xpu image
      community.docker.docker_image:
        name: "{{ ollama_arc_image }}"
        source: build
        force_source: "{{ force_update | default(false) }}"
        build:
          path: "{{ ollama_arc_build_repo_dest }}/{{ ollama_arc_build_context_subdir }}"
          args: "{{ ollama_arc_build_args }}" # Correct parameter name is 'args'
          nocache: "{{ git_clone_result.changed or force_update | default(false) }}" # Rebuild if repo changed or forced
          rm: "{{ ollama_arc_build_rm }}"
          pull: true # Attempt to pull base images
        state: present

    - name: Build the ipex-llm-serving-xpu image
      community.docker.docker_image:
        name: "{{ ipex_vllm_image }}"
        source: build
        force_source: "{{ force_update | default(false) }}"
        build:
          path: "{{ ollama_arc_build_repo_dest }}/{{ ipex_vllm_build_context_subdir }}"
          args: "{{ ollama_arc_build_args }}" # Correct parameter name is 'args'
          nocache: "{{ git_clone_result.changed or force_update | default(false) }}" # Rebuild if repo changed or forced
          rm: "{{ ollama_arc_build_rm }}"
          pull: true # Attempt to pull base images
        state: present

    - name: Build official vllm xpu image
      ansible.builtin.shell:
        cmd: >
          docker build
          -t {{ vllm_image }}
          -f docker/Dockerfile.xpu
          --shm-size=4g
          --pull
          .
        chdir: "{{ vllm_repo_dest }}"
      environment:
        DOCKER_BUILDKIT: "1"
      become: true # Docker build often needs elevated privileges
      register: vllm_build_result
      changed_when: "'Successfully built' in vllm_build_result.stdout or 'Successfully tagged' in vllm_build_result.stdout" # Check for success messages

- name: Define ollama container environment variables
  ansible.builtin.set_fact:
    ollama_arc_base_env_vars:
      no_proxy: "{{ ollama_arc_no_proxy }}"
      DEVICE: "{{ ollama_arc_device_type }}"
      OLLAMA_DEBUG: "0"
      OLLAMA_HOST: "0.0.0.0" # Listen on all interfaces
      OLLAMA_MODELS: /models # Set Ollama's model storage path
      OLLAMA_NUM_GPU: "999" # Use all available GPUs for Ollama (as string)
      OLLAMA_FLASH_ATTENTION: "1" # Not supported yet by ipex-llm
      OLLAMA_KV_CACHE_TYPE: "q8_0" # Not supported yet by ipex-llm
      OLLAMA_CONTEXT_LENGTH: "16384" # "32768"
      IPEX_LLM_NUM_CTX: "16384" # "32768"
      SYCL_CACHE_PERSISTENT: "1"
      SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS: "1"
      ZES_ENABLE_SYSMAN: "1" # Required for sysman metrics (as string)
      ONEAPI_DEVICE_SELECTOR: "level_zero:1"

- name: Combine environment variables
  ansible.builtin.set_fact:
    ollama_arc_env_vars: "{{ ollama_arc_base_env_vars }}"

- name: Start the ipex-llm-inference-cpp-xpu container
  community.docker.docker_container:
    name: "{{ ollama_arc_container_name }}"
    image: "{{ ollama_arc_image }}"
    ulimits:
      - 'memlock:-1:-1'
    command: >
      bash -c "mkdir -p /llm/ollama &&
               cd /llm/ollama &&
               mkdir -p /llm/llama-cpp &&
               cd /llm/llama-cpp &&
               source ipex-llm-init --gpu --device {{ ollama_arc_device_type }} &&
               init-ollama &&
               exec ./ollama serve
               # init-llama-cpp &&
               # exec ./llama-server -m /models/qwen2.5-0.5b-instruct-q4_k_m.gguf"
    state: started
    detach: true
    network_mode: host
    devices:
      - /dev/dri/
    volumes:
      - "{{ ollama_arc_models_path }}:/models"
    env: "{{ ollama_arc_env_vars|combine({\"OLLAMA_PORT\": \"11434\"}) }}"
    memory: "{{ ollama_arc_memory }}"
    shm_size: "{{ ollama_arc_shm_size }}"
    restart_policy: "{{ ollama_arc_restart_policy }}"
  become: true

- name: Ensure vLLM script destination directory exists
  ansible.builtin.file:
    path: "{{ vllm_script_host_path | dirname }}"
    state: directory
    mode: "0755"
  become: true

- name: Copy vLLM start script to host
  ansible.builtin.copy:
    src: start-vllm-service.sh # Assumes file is in roles/ollama_arc/files/
    dest: "{{ vllm_script_host_path }}"
    mode: "0755"
  become: true

- name: Define vLLM container environment variables
  ansible.builtin.set_fact:
    vllm_env_vars:
      MODEL_PATH: "{{ vllm_model_path | default('/llm/models/default_model') }}" # Example default
      SERVED_MODEL_NAME: "{{ vllm_served_model_name | default('default_served_name') }}" # Example default
      TENSOR_PARALLEL_SIZE: "{{ vllm_tensor_parallel_size | default(1) }}"
      MAX_NUM_SEQS: "{{ vllm_max_num_seqs | default(256) }}"
      MAX_NUM_BATCHED_TOKENS: "{{ vllm_max_num_batched_tokens | default(3000) }}"
      MAX_MODEL_LEN: "{{ vllm_max_model_len | default(2000) }}"
      LOAD_IN_LOW_BIT: "{{ vllm_load_in_low_bit | default('fp8') }}"
      PORT: "{{ vllm_port | default(8000) }}"
      VLLM_QUANTIZATION: "{{ vllm_quantization | default('') }}"
      CACHE_DTYPE: "{{ vllm_cache_dtype | default('') }}"
      DOWNLOAD_DIR: "{{ vllm_download_dir | default('/llm/models') }}"
      PREFIX_CACHING: "{{ vllm_prefix_caching | default('0') }}"
      no_proxy: "{{ ollama_arc_no_proxy | default('localhost,127.0.0.1') }}" # Reuse existing no_proxy or define vllm_no_proxy
      # Add other necessary env vars like SYCL, CCL etc. if needed, inheriting from defaults or explicitly setting
      SYCL_CACHE_PERSISTENT: "1"
      SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS: "1" # Or 2 based on script
      FI_PROVIDER: "shm"
      CCL_ATL_TRANSPORT: "ofi"
      CCL_ZE_IPC_EXCHANGE: "sockets"
      CCL_ATL_SHM: "1"
      USE_XETLA: "OFF"
      TORCH_LLM_ALLREDUCE: "0"
      CCL_SAME_STREAM: "1"
      CCL_BLOCKING_WAIT: "0"

- name: Start the ipex-llm-serving-xpu container
  community.docker.docker_container:
    name: "{{ vllm_container_name | default('ipex-llm-serving-xpu-container') }}"
    image: "{{ ipex_vllm_image }}" # Use the image built earlier
    state: started
    detach: true
    privileged: true # As per example command
    network_mode: host
    devices:
      - /dev/dri:/dev/dri # Ensure device mapping if not covered by privileged
    volumes:
      - "{{ vllm_models_path | default(ollama_arc_models_path) }}:/llm/models:rw" # Mount models dir
      - "{{ vllm_huggingface_path | default(ansible_user_dir + '/.huggingface') }}:/root/.huggingface:rw" # Mount Hugging Face cache
      - "{{ vllm_script_host_path }}:{{ vllm_script_container_path | default('/llm/start-vllm-service.sh') }}:ro" # Mount start script read-only
    env: "{{ vllm_env_vars }}"
    entrypoint: ["/bin/bash", "{{ vllm_script_container_path | default('/llm/start-vllm-service.sh') }}"] # Use script as entrypoint
    memory: "{{ vllm_memory | default('32g') }}"
    shm_size: "{{ vllm_shm_size | default('16g') }}"
    restart_policy: "{{ vllm_restart_policy | default('unless-stopped') }}"
  become: true
