---
# tasks file for ollama-arc

- name: Ensure models directory exists
  ansible.builtin.file:
    path: "{{ ollama_arc_models_path }}"
    state: directory
    mode: "0755"
  become: true

# Block for building the image locally (always runs)
- name: Build ipex-llm-inference-cpp-xpu image from source
  become: true # Often needed for git checkout destination and docker build
  block:
    - name: Ensure git package is present for cloning
      ansible.builtin.package:
        name: git
        state: latest

    - name: Ensure build destination directory exists
      ansible.builtin.file:
        path: "{{ ollama_arc_build_repo_dest | ansible.builtin.dirname }}" # Ensure parent dir exists
        state: directory
        mode: "0755"

    - name: Clone or update the ipex-llm repository
      ansible.builtin.git:
        repo: "{{ ollama_arc_build_repo_url }}"
        dest: "{{ ollama_arc_build_repo_dest }}"
        version: "{{ ollama_arc_build_repo_version }}"
        force: true # Ensure the correct version is checked out
        depth: 1 # Perform a shallow clone

    - name: Build the ipex-llm-inference-cpp-xpu image
      community.docker.docker_image:
        name: "{{ ollama_arc_image }}"
        source: build
        force_source: "{{ force_update | default(false) }}"
        build:
          path: "{{ ollama_arc_build_repo_dest }}/{{ ollama_arc_build_context_subdir }}"
          args: "{{ ollama_arc_build_args }}" # Correct parameter name is 'args'
          nocache: "{{ git_clone_result.changed or force_update | default(false) }}" # Rebuild if repo changed or forced
          rm: "{{ ollama_arc_build_rm }}"
          pull: true # Attempt to pull base images
        state: present
      # Note: This build can take a significant amount of time

- name: Define base container environment variables
  ansible.builtin.set_fact:
    ollama_arc_base_env_vars:
      no_proxy: "{{ ollama_arc_no_proxy }}"
      DEVICE: "{{ ollama_arc_device_type }}"
      OLLAMA_HOST: "0.0.0.0" # Listen on all interfaces
      OLLAMA_MODELS: /models # Set Ollama's model storage path
      OLLAMA_NUM_GPU: "999" # Use all available GPUs for Ollama (as string)
      ZES_ENABLE_SYSMAN: "1" # Required for sysman metrics (as string)

- name: Conditionally add benchmark model environment variable
  ansible.builtin.set_fact:
    ollama_arc_bench_env_var: "{% if ollama_arc_benchmark_model | length > 0 %}{'bench_model': '{{ ollama_arc_benchmark_model
      }}'}{% else %}{}{% endif %}"

- name: Combine environment variables
  ansible.builtin.set_fact:
    ollama_arc_env_vars: "{{ ollama_arc_base_env_vars | combine(ollama_arc_bench_env_var) }}"

- name: Start the ipex-llm-inference-cpp-xpu container
  community.docker.docker_container:
    name: "{{ ollama_arc_container_name }}"
    image: "{{ ollama_arc_image }}"
    command: >
      bash -c "mkdir -p /llm/ollama &&
               cd /llm/ollama &&
               source ipex-llm-init --gpu --device {{ ollama_arc_device_type }} &&
               init-ollama &&
               exec ./ollama serve"
    state: started
    detach: true
    network_mode: host
    devices:
      - /dev/dri:/dev/dri
    volumes:
      - "{{ ollama_arc_models_path }}:/models"
    env: "{{ ollama_arc_env_vars }}"
    memory: "{{ ollama_arc_memory }}"
    shm_size: "{{ ollama_arc_shm_size }}"
    restart_policy: "{{ ollama_arc_restart_policy }}"
  become: true
