---
# tasks file for llm_inference

- name: Ensure Ollama models directory exists
  ansible.builtin.file:
    path: "{{ ollama_path }}"
    state: directory
    mode: "0755"
  become: true

- name: Start the Ollama container
  community.docker.docker_container:
    name: "{{ ollama_container_name }}"
    image: "{{ ollama_image }}"
    pull: true
    state: started
    restart_policy: "{{ ollama_restart_policy }}"
    ports:
      - "{{ ollama_port }}:{{ ollama_port }}"
    volumes:
      - "{{ ollama_path }}:/root/.ollama"
    device_requests:
      - driver: nvidia
        device_ids: ["{{ blackwell_gpu_uuid }}"]
        capabilities:
          - [gpu]
    env:
      OLLAMA_HOST: "0.0.0.0"
      OLLAMA_FLASH_ATTENTION: "1"
      OLLAMA_KV_CACHE_TYPE: "q8_0"
      OLLAMA_NUM_PARALLEL: "{{ ollama_num_parallel }}"
      OLLAMA_MAX_LOADED_MODELS: "{{ ollama_max_loaded_models }}"
  become: true

- name: Ensure llama-swap config directory exists
  ansible.builtin.file:
    path: "{{ llamaswap_config_path }}"
    state: directory
    mode: "0755"
  become: true

- name: Ensure llama-swap config directory exists
  ansible.builtin.file:
    path: "{{ llamaswap_config_path }}/docker/sglang"
    state: directory
    mode: "0755"
  become: true

- name: Template sglang Dockerfile
  ansible.builtin.template:
    src: Dockerfile.sglang.j2
    dest: "{{ llamaswap_config_path }}/docker/sglang/Dockerfile"
    mode: "0644"
  become: true
  register: sglang_dockerfile

- name: Build custom sglang image
  community.docker.docker_image:
    name: sglang-custom
    tag: local
    build:
      path: "{{ llamaswap_config_path }}/docker/sglang"
      pull: true
    source: build
    force_source: "{{ sglang_dockerfile.changed or force_rebuild | default(false) }}"
  become: true

- name: Create llama-swap config file
  ansible.builtin.template:
    src: llamaswap_config.yaml.j2
    dest: "{{ llamaswap_config_path }}/config.yaml"
    mode: "0644"
  become: true
  notify: Restart llama-swap

- name: Start the llama-swap container
  community.docker.docker_container:
    name: "{{ llamaswap_container_name }}"
    image: "{{ llamaswap_image }}"
    image_name_mismatch: recreate
    pull: true
    state: started
    restart_policy: unless-stopped
    ports:
      - "{{ llamaswap_port }}:8080"
    volumes:
      - "{{ llamaswap_config_path }}/config.yaml:/app/config.yaml"
      - "{{ llamaswap_config_path }}/cache:/root/.cache"
      - "/var/run/docker.sock:/var/run/docker.sock"
      - "/usr/bin/docker:/usr/bin/docker"
    device_requests:
      - driver: nvidia
        count: -1
        capabilities:
          - [gpu]
    capabilities:
      - SYS_NICE
    env:
      LLAMASWAP_RESTART_TOKEN: "{{ ansible_facts.date_time.epoch }}"
    labels:
      homepage.group: "{{ services['llama-swap'].group }}"
      homepage.name: "{{ services['llama-swap'].name }}"
      homepage.href: "https://llama-swap.lan"
      homepage.icon: llama.png
      homepage.server: "{{ inventory_hostname }}"
      homepage.container: "{{ llamaswap_container_name }}"
  become: true
