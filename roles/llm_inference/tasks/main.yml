---
# tasks file for llm_inference

- name: Ensure models directory exists
  ansible.builtin.file:
    path: "{{ ollama_path }}"
    state: directory
    mode: "0755"
  become: true

- name: Start the Ollama container
  community.docker.docker_container:
    name: "{{ ollama_container_name }}"
    image: "{{ ollama_image }}"
    pull: true
    state: started
    restart_policy: "{{ ollama_restart_policy }}"
    ports:
      - "{{ ollama_port }}:11434"
    volumes:
      - "{{ ollama_path }}:/root/.ollama"
    device_requests:
      - driver: nvidia
        count: -1
        capabilities:
          - [gpu]
    env:
      OLLAMA_HOST: "0.0.0.0"
      OLLAMA_FLASH_ATTENTION: "1"
      OLLAMA_KV_CACHE_TYPE: "q8_0"
      OLLAMA_CONTEXT_LENGTH: "40960"
      OLLAMA_NUM_PARALLEL: "2"
      OLLAMA_MAX_LOADED_MODELS: "2"
  become: true
