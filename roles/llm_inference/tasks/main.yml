---
# tasks file for llm_inference

- name: Stop and remove old Ollama container if exists
  community.docker.docker_container:
    name: ollama
    state: absent
  become: true
  ignore_errors: true

- name: Ensure llama-swap config directory exists
  ansible.builtin.file:
    path: "{{ llamaswap_config_path }}"
    state: directory
    mode: "0755"
  become: true

- name: Ensure llama-swap cache directory exists
  ansible.builtin.file:
    path: "{{ llamaswap_config_path }}/cache"
    state: directory
    mode: "0777"
    owner: "1000"
    group: "1000"
  become: true

- name: Create llama-swap config file
  ansible.builtin.template:
    src: llamaswap_config.yaml.j2
    dest: "{{ llamaswap_config_path }}/config.yaml"
    mode: "0644"
  become: true
  notify: Restart llama-swap

- name: Start the llama-swap container
  community.docker.docker_container:
    name: "{{ llamaswap_container_name }}"
    image: ghcr.io/mostlygeek/llama-swap:cuda
    image_name_mismatch: recreate
    pull: true
    state: started
    restart_policy: unless-stopped
    ports:
      - "{{ llamaswap_port }}:8080"
    volumes:
      - "{{ llamaswap_config_path }}/config.yaml:/app/config.yaml"
      - "{{ llamaswap_config_path }}/cache:/app/.cache"
    device_requests:
      - driver: nvidia
        count: -1
        capabilities:
          - [gpu]
    env:
      LLAMASWAP_RESTART_TOKEN: "{{ ansible_facts.date_time.epoch }}"
    labels:
      homepage.group: "{{ services['llama-swap'].group }}"
      homepage.name: "{{ services['llama-swap'].name }}"
      homepage.href: "https://llama-swap.lan"
      homepage.icon: llama.png
      homepage.server: "{{ inventory_hostname }}"
      homepage.container: "{{ llamaswap_container_name }}"
  become: true
