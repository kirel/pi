# llama-swap configuration for Qwen3-VL-8B-Instruct-GGUF
# Models to load and serve

models:
  qwen3-vl:
    # Load model from HuggingFace with optimal parameters for Instruct variant
    cmd: >
      llama-server
      -hf {{ llamaswap_model_hf }}
      --port ${PORT}
      --ctx-size 8192
      --flash-attn
      --n-gpu-layers 99
      --top-p 0.8
      --top-k 20
      --temp 0.7
      --min-p 0.0
      --presence-penalty 1.5
      --jinja
    # GPU settings for RTX 3090
    env:
      - "GGML_CUDA_MOUNT=/usr/lib/x86_64-linux-gnu/libcudart.so.12"

# Global settings
server:
  # Allow CORS for web interfaces
  cors: true
  # Set timeout
  timeout: 3600
