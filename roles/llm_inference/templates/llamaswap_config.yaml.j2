# llama-swap configuration
# Models to load and serve

models:
  {% for model in llamaswap_models -%}
  {{ model.alias }}:
    cmd: |
      {{ model.cmd | indent(width=6, first=True) }}
    env:
      - "GGML_CUDA_MOUNT=/usr/lib/x86_64-linux-gnu/libcudart.so.12"
  {% endfor %}

{% if llamaswap_model_groups is defined %}
groups:
  {% for group_name, group_config in llamaswap_model_groups.items() %}

  "{{ group_name }}":
    {% if group_config.persistent is defined %}persistent: {{ group_config.persistent | lower }}{% endif +%}
    {% if group_config.swap is defined %}swap: {{ group_config.swap | lower }}{% endif +%}
    {% if group_config.exclusive is defined %}exclusive: {{ group_config.exclusive | lower }}{% endif +%}

    members:
      {% for model in llamaswap_models if group_name == model.group -%}
      - "{{ model.alias }}"
      {% endfor %}
  {% endfor %}
{% endif %}

# Global settings
server:
  # Allow CORS for web interfaces
  cors: true
  # Set timeout
  timeout: 3600

# Preload models on startup
hooks:
  on_startup:
    preload:
      - GPT-OSS-20B-F16
