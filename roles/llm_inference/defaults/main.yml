---
# defaults file for llm_inference

# --- Ollama Settings ---
ollama_image: "ollama/ollama:latest"
ollama_container_name: "ollama"
ollama_port: "11434"
ollama_path: "{{ ollama_host_path }}" # Default path on host for ollama models
ollama_restart_policy: "unless-stopped"

# --- llama-swap Settings ---
llamaswap_container_name: "llama-swap"
llamaswap_port: "8080"
llamaswap_config_path: "{{ llamaswap_host_path }}"
llamaswap_preload: []

# --- Context Length Settings ---
ollama_num_parallel: "2"
ollama_max_loaded_models: "2"
