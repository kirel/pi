---
# defaults file for llm_inference

# --- Ollama Settings ---
ollama_image: "ollama/ollama:latest"
ollama_container_name: "ollama"
ollama_port: "{{ ports.ailab_ubuntu.ollama }}"
llamaswap_port: "{{ ports.ailab_ubuntu.llamaswap }}"
llamaswap_config_path: "{{ llamaswap_host_path }}"
llamaswap_preload: []

# --- Context Length Settings ---
ollama_num_parallel: "2"
ollama_max_loaded_models: "2"
