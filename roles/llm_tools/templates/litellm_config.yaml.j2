model_list:
  # Primary home model - Local qwen3-vl:8b with optimal context
  - model_name: home
    litellm_params:
      model: "ollama_chat/qwen3-vl:8b-thinking-q8_0"
      api_base: "http://{{ ollama_host }}:{{ ollama_port }}"
      # Set keepalive to -1 (infinite) to keep model loaded
      extra_body:
        keep_alive: -1
        # Set optimal context for RTX 3090 (128,256 tokens = 100% GPU)
        options:
          num_ctx: 128256
    model_info:
      supports_function_calling: true
      supports_vision: true

  # Fallback to remote model when local is unavailable
  - model_name: home-remote
    litellm_params:
      model: anthropic/claude-3-5-haiku-20241022
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # Wildcard for other providers
  - model_name: "*"
    litellm_params:
      model: "*"

router_settings:
  # Fallback from home to home-remote when local model fails
  fallbacks: [{"home": ["home-remote"]}]
  redis_host: "{{ litellm_redis_container_name }}"
  redis_port: "{{ litellm_redis_port_container }}"

litellm_settings:
  success_callback: ["langfuse"]

general_settings:
  master_key: "{{ litellm_master_key }}"

