model_list:
  # Primary home model - Local Qwen3-VL via llama-swap
  - model_name: home
    litellm_params:
      model: "openai/{{ llamaswap_model_alias }}"
      api_base: "http://{{ ollama_host }}:{{ llamaswap_port }}/v1"
      api_key: "llama-swap"
      # Set keepalive to -1 (infinite) to keep model loaded
      extra_body:
        keep_alive: -1
    model_info:
      supports_function_calling: true
      supports_vision: true

  # Fallback to remote model when local is unavailable
  - model_name: home-remote
    litellm_params:
      model: anthropic/claude-haiku-4-5
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # Wildcard for other providers
  - model_name: "*"
    litellm_params:
      model: "*"

router_settings:
  # Fallback from home to home-remote when local model fails
  fallbacks: [{"home": ["home-remote"]}]
  redis_host: "{{ litellm_redis_container_name }}"
  redis_port: "{{ litellm_redis_port_container }}"

litellm_settings:
  success_callback: ["langfuse"]

general_settings:
  master_key: "{{ litellm_master_key }}"
