model_list:
  # Primary home model - Local Qwen3-VL via llama-swap
  {% for model in llamaswap_models -%}
  - model_name: {{ model.alias }}
    litellm_params:
      model: "openai/{{ model.alias }}"
      api_base: "http://{{ ollama_host }}:{{ llamaswap_port }}/v1"
      api_key: "llama-swap"
      {% if model.allowed_openai_params is defined -%}
      allowed_openai_params: {{ model.allowed_openai_params }}
      {% endif -%}
    model_info:
      supports_function_calling: {{ model.supports_function_calling | lower }}
      supports_vision: {{ model.supports_vision | lower }}
  {% endfor %}

  # Fallback to remote model when local is unavailable
  - model_name: home-remote
    litellm_params:
      model: anthropic/claude-haiku-4-5
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # Wildcard for other providers
  - model_name: "*"
    litellm_params:
      model: "*"

router_settings:
  model_group_alias: {{ model_group_alias }}
  # Fallback from home to home-remote when local model fails
  fallbacks: [{"home": ["home-remote"]}]
  redis_host: "{{ litellm_redis_container_name }}"
  redis_port: "{{ litellm_redis_port_container }}"

litellm_settings:
  success_callback: ["langfuse"]

general_settings:
  master_key: "{{ litellm_master_key }}"
