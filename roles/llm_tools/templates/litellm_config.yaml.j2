model_list:
  # Local models via llama-swap - base models and their variants
  {% for model in llamaswap_models -%}
  # Base model
  - model_name: {{ model.alias }}
    litellm_params:
      model: "openai/{{ model.alias }}"
      api_base: "http://{{ ollama_host }}:{{ llamaswap_port }}/v1"
      api_key: "llama-swap"
      {% if model.allowed_openai_params is defined -%}
      allowed_openai_params: {{ model.allowed_openai_params }}
      {% endif -%}
    model_info:
      supports_function_calling: {{ model.supports_function_calling | lower }}
      supports_vision: {{ model.supports_vision | lower }}

  # Model variants (if any)
  {% if model.variants is defined -%}
  {% for variant_name, variant_params in model.variants.items() -%}
  - model_name: {{ model.alias }}-{{ variant_name }}
    litellm_params:
      model: "openai/{{ model.alias }}"
      api_base: "http://{{ ollama_host }}:{{ llamaswap_port }}/v1"
      api_key: "llama-swap"
      {% if model.allowed_openai_params is defined -%}
      allowed_openai_params: {{ model.allowed_openai_params }}
      {% endif -%}
      {{ variant_params | to_nice_yaml(indent=2) | indent(6, false) }}
    model_info:
      supports_function_calling: {{ model.supports_function_calling | lower }}
      supports_vision: {{ model.supports_vision | lower }}
  {% endfor %}
  {% endif %}
  {% endfor %}

  # Standard Ollama models
{% if ollama_models is defined %}
{% for model in ollama_models %}
  - model_name: {{ model.alias }}
    litellm_params:
      model: "ollama_chat/{{ model.name }}"
      api_base: "http://{{ ollama_host }}:{{ ollama_port }}"
{% if model.num_ctx is defined %}
      num_ctx: {{ model.num_ctx }}
{% endif %}
{% endfor %}
{% endif %}

  # Fallback to remote model when local is unavailable
  - model_name: home-remote
    litellm_params:
      model: anthropic/claude-haiku-4-5
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # Wildcard for other providers
  - model_name: "*"
    litellm_params:
      model: "*"

router_settings:
  model_group_alias: {{ model_group_alias }}
  # Fallback from home to home-remote when local model fails
  fallbacks: {{ litellm_fallbacks }}
  redis_host: "{{ litellm_redis_container_name }}"
  redis_port: "{{ litellm_redis_port_container }}"

litellm_settings:
  drop_params: true
  master_key: "{{ litellm_master_key }}"
  cache: true
  cache_params:
    type: redis
    host: "{{ litellm_redis_container_name }}"
    port: "{{ litellm_redis_port_container }}"
  callbacks: ["arize_phoenix"]

general_settings:
  store_model_in_db: true
  store_prompts_in_spend_logs: true
