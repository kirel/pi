{% set ollama_models_map = [
  { "alias": "Qwen3-8B-GGUF:Q4_K_XL", "ollama_id": "hf.co/unsloth/Qwen3-8B-GGUF:Q4_K_XL" },
  { "alias": "Qwen3-8B-GGUF:Q6_K_XL", "ollama_id": "hf.co/unsloth/Qwen3-8B-GGUF:Q6_K_XL" },
  { "alias": "Qwen3-14B-GGUF:Q6_K_XL", "ollama_id": "hf.co/unsloth/Qwen3-14B-GGUF:Q6_K_XL" }
] -%}
model_list:
  - model_name: home-nano
    litellm_params:
      model: openrouter/openai/gpt-4.1-nano
      api_key: "os.environ/OPENROUTER_API_KEY"
  - model_name: home-mini
    litellm_params:
      model: openrouter/openai/gpt-4.1-mini
      api_key: "os.environ/OPENROUTER_API_KEY"
  - model_name: home-maxi
    litellm_params:
      model: openrouter/openai/gpt-4.1
      api_key: "os.environ/OPENROUTER_API_KEY"

  - model_name: home-local
    litellm_params:
      model: "openai/hf.co/unsloth/Qwen3-14B-GGUF:Q6_K_XL"
      api_base: "http://{{ ollama_host }}:{{ ollama_port }}/v1" # Points directly to Ollama's OpenAI-compatible endpoint
      api_key: ollama
    model_info:
      supports_function_calling: true

  - model_name: home-ollama
    litellm_params:
      model: "ollama_chat/hf.co/unsloth/Qwen3-14B-GGUF:Q6_K_XL"
      api_base: "http://{{ ollama_host }}:{{ ollama_port }}"
    model_info:
      supports_function_calling: true

  - model_name: "*"
    litellm_params:
      model: "*"

  {% for model_info in ollama_models_map -%}
  - model_name: "{{ model_info.alias }}"
    litellm_params:
      model: "openai/{{ model_info.ollama_id }}" # Model ID for Ollama, but openai protocol
      api_base: "http://{{ ollama_host }}:{{ ollama_port }}/v1" # Points directly to Ollama's OpenAI-compatible endpoint
      api_key: ollama
    model_info:
      supports_function_calling: true
  {% endfor %}

router_settings:
  fallbacks: [{"home": ["home-local"]}]

litellm_settings:
  success_callback: ["langfuse"]

general_settings:
  master_key: "{{ litellm_master_key }}"
