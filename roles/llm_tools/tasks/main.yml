---
# tasks file for llm_tools

- name: Create LiteLLM config directory
  ansible.builtin.file:
    path: "{{ litellm_proxy_config_dir_host }}"
    state: directory
    mode: "0755"
  become: true

- name: Define LiteLLM Proxy config path fact
  ansible.builtin.set_fact:
    litellm_proxy_config_full_host_path: "{{ litellm_proxy_config_dir_host }}/{{ litellm_proxy_config_filename }}"

- name: Template LiteLLM Proxy config.yaml
  ansible.builtin.template:
    src: litellm_config.yaml.j2
    dest: "{{ litellm_proxy_config_full_host_path }}"
    mode: "0644"
  notify: Restart LiteLLM Proxy service

- name: Ensure Open WebUI config directory exists
  ansible.builtin.file:
    path: "{{ open_webui_config_folder }}"
    state: directory
    mode: "0755"
    owner: "{{ uid }}"
    group: "{{ uid }}"
  become: true

- name: Deploy all LiteLLM services using Docker Compose
  community.docker.docker_compose_v2:
    project_name: litellm
    state: present
    definition:
      version: '3.8'
      services:
        postgres:
          image: "{{ litellm_db_image }}"
          pull_policy: always
          container_name: "{{ litellm_db_container_name }}"
          ports:
            - "{{ litellm_db_port_host }}:{{ litellm_db_port_container }}"
          volumes:
            - "{{ litellm_db_data_dir }}:/var/lib/postgresql/data"
          environment:
            POSTGRES_USER: "{{ litellm_db_user }}"
            POSTGRES_PASSWORD: "{{ litellm_db_password }}"
            POSTGRES_DB: "{{ litellm_db_name }}"
          healthcheck:
            test: ["CMD-SHELL", "pg_isready -U {{ litellm_db_user }}"]
            interval: 5s
            timeout: 5s
            retries: 10
          restart: unless-stopped
          networks:
            - llm-network

        redis:
          image: "{{ litellm_redis_image }}"
          pull_policy: always
          container_name: "{{ litellm_redis_container_name }}"
          ports:
            - "{{ litellm_redis_port_host }}:{{ litellm_redis_port_container }}"
          volumes:
            - "{{ litellm_redis_data_dir }}:/data"
          command: redis-server --appendonly yes
          restart: unless-stopped
          networks:
            - llm-network

        litellm-proxy:
          image: "{{ litellm_proxy_image }}"
          pull_policy: always
          container_name: "{{ litellm_proxy_container_name }}"
          ports:
            - "{{ litellm_proxy_port_host }}:{{ litellm_proxy_port_container }}"
          volumes:
            - "{{ litellm_proxy_config_full_host_path }}:/app/config.yaml:ro"
            - /etc/ssl/certs:/etc/ssl/certs:ro
            - "{{ caddy_data_folder }}/caddy/pki/authorities/local:/opt/custom-certificates:ro"
          environment:
            PHOENIX_COLLECTOR_ENDPOINT: "{{ phoenix_host_for_litellm }}"
            LITELLM_MASTER_KEY: "{{ litellm_master_key }}"
            DATABASE_URL: "{{ litellm_database_url }}"
            REDIS_HOST: "{{ litellm_redis_container_name }}"
            REDIS_PORT: "{{ litellm_redis_port_container }}"
            GEMINI_API_KEY: "{{ GEMINI_API_KEY }}"
            OPENROUTER_API_KEY: "{{ OPENROUTER_API_KEY }}"
            ANTHROPIC_API_KEY: "{{ ANTHROPIC_API_KEY }}"
            NODE_EXTRA_CA_CERTS: /etc/ssl/certs/ca-certificates.crt
          command: ["--config", "/app/config.yaml", "--port", "{{ litellm_proxy_port_container }}"]
          depends_on:
            postgres:
              condition: service_healthy
            redis:
              condition: service_started
          restart: unless-stopped
          networks:
            - llm-network
          labels:
            wud.tag.include: main
            wud.watch.digest: "true"
            homepage.group: "{{ services['litellm-ui'].group }}"
            homepage.name: "{{ services['litellm-ui'].name }}"
            homepage.href: https://litellm-ui.lan
            homepage.icon: "{{ services['litellm-ui'].icon | default('litellm.png') }}"
            homepage.server: "{{ inventory_hostname }}"
            homepage.container: "{{ litellm_proxy_container_name }}"

        # mcp-proxy removed - using MetaMCP instead
        # mcp-proxy:
        #   image: "{{ mcp_proxy_image_custom }}"
        #   container_name: "{{ mcp_proxy_container_name }}"
        #   ports:
        #     - "{{ mcp_proxy_service_port }}:{{ mcp_proxy_service_port }}"
        #   volumes:
        #     - "{{ mcp_proxy_config_full_host_path }}:/app/servers.json:ro"
        #   command: "mcp-proxy --port {{ mcp_proxy_service_port }} --host 0.0.0.0 --named-server-config /app/servers.json --pass-environment"
        #   restart: unless-stopped
        #   networks:
        #     - llm-network

        open-webui:
          image: ghcr.io/open-webui/open-webui:main
          pull_policy: always
          container_name: open-webui
          ports:
            - "{{ open_webui_http_port }}:8080"
          environment:
            OLLAMA_BASE_URL: "{{ ollama_base_url_for_webui }}"
            WEBUI_SECRET_KEY: t0p-s3cr3t
            SSL_CERT_FILE: "/etc/ssl/certs/ca-certificates.crt"
            PUID: "{{ uid }}"
            PGID: "{{ uid }}"
          volumes:
            - "{{ open_webui_config_folder }}:/app/backend/data"
            - "/etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt:ro"
          labels:
            wud.tag.include: main
            wud.watch.digest: "true"
            homepage.group: "{{ services['open-webui'].group }}"
            homepage.name: "{{ services['open-webui'].name }}"
            homepage.href: https://open-webui.lan
            homepage.icon: openai.png
            homepage.server: "{{ inventory_hostname }}"
            homepage.container: open-webui
          restart: unless-stopped
          networks:
            - llm-network
          depends_on:
            litellm-proxy:
              condition: service_started
            # mcp-proxy removed - using MetaMCP instead

        google-workspace-mcp:
          image: "{{ google_workspace_mcp_image }}"
          pull_policy: always
          container_name: "{{ google_workspace_mcp_container_name }}"
          ports:
            - "{{ google_workspace_mcp_port_host }}:{{ google_workspace_mcp_port_container }}"
          volumes:
            - /etc/ssl/certs:/etc/ssl/certs:ro
            - "{{ caddy_data_folder }}/caddy/pki/authorities/local:/opt/custom-certificates:ro"
          environment:
            GOOGLE_OAUTH_CLIENT_ID: "{{ google_workspace_oauth_client_id }}"
            GOOGLE_OAUTH_CLIENT_SECRET: "{{ google_workspace_oauth_client_secret }}"
            MCP_ENABLE_OAUTH21: "true"
            WORKSPACE_MCP_STATELESS_MODE: "true"
            WORKSPACE_EXTERNAL_URL: "https://google-workspace-mcp.net"
            REQUESTS_CA_BUNDLE: /etc/ssl/certs/ca-certificates.crt
            CURL_CA_BUNDLE: /etc/ssl/certs/ca-certificates.crt
          command:
            - "uvx"
            - "workspace-mcp"
            - "--transport"
            - "streamable-http"
          restart: unless-stopped
          networks:
            - llm-network

      networks:
        llm-network:
          driver: bridge
  become: true
