---
# tasks file for llm_tools

- name: Create LiteLLM config directory
  ansible.builtin.file:
    path: "{{ litellm_proxy_config_dir_host }}"
    state: directory
    mode: "0755"
  become: true

- name: Define LiteLLM Proxy config path fact
  ansible.builtin.set_fact:
    litellm_proxy_config_full_host_path: "{{ litellm_proxy_config_dir_host }}/{{ litellm_proxy_config_filename }}"

- name: Template LiteLLM Proxy config.yaml
  ansible.builtin.template:
    src: litellm_config.yaml.j2
    dest: "{{ litellm_proxy_config_full_host_path }}"
    mode: "0644"
  notify: Restart LiteLLM Proxy service

# Block for MCP-Proxy setup
- name: MCP-Proxy Setup
  become: true
  block:
    - name: Ensure old mcp-bridge container is removed to free up port
      community.docker.docker_container:
        name: "mcp-bridge-container"
        state: absent

    - name: Define MCP-Proxy related facts
      ansible.builtin.set_fact:
        mcp_proxy_config_full_host_path: "{{ mcp_proxy_config_dir_host }}/{{ mcp_proxy_config_filename }}"
        mcp_proxy_build_context_path: "{{ mcp_proxy_config_dir_host }}/build"

    - name: Ensure mcp-proxy build directory exists
      ansible.builtin.file:
        path: "{{ mcp_proxy_build_context_path }}"
        state: directory
        mode: "0755"

    - name: Template custom Dockerfile for mcp-proxy
      ansible.builtin.template:
        src: Dockerfile.mcp-proxy.j2
        dest: "{{ mcp_proxy_build_context_path }}/Dockerfile"
        mode: "0644"
      register: dockerfile_result

    - name: Build the custom mcp-proxy image
      community.docker.docker_image:
        name: "{{ mcp_proxy_image_custom }}"
        source: build
        force_source: "{{ dockerfile_result.changed or force_update | default(false) }}"
        build:
          path: "{{ mcp_proxy_build_context_path }}"
          pull: true
          rm: true
        state: present

    - name: Ensure mcp-proxy config directory exists on host
      ansible.builtin.file:
        path: "{{ mcp_proxy_config_dir_host }}"
        state: directory
        mode: "0755"

    - name: Template mcp-proxy servers.json
      ansible.builtin.template:
        src: mcp_proxy_servers.json.j2
        dest: "{{ mcp_proxy_config_full_host_path }}"
        mode: "0644"
      notify: Restart MCP-Proxy service

- name: Deploy all LiteLLM services using Docker Compose
  community.docker.docker_compose_v2:
    project_name: litellm
    state: present
    definition:
      version: '3.8'
      services:
        postgres:
          image: "{{ litellm_db_image }}"
          container_name: "{{ litellm_db_container_name }}"
          ports:
            - "{{ litellm_db_port_host }}:{{ litellm_db_port_container }}"
          volumes:
            - "{{ litellm_db_data_dir }}:/var/lib/postgresql/data"
          environment:
            POSTGRES_USER: "{{ litellm_db_user }}"
            POSTGRES_PASSWORD: "{{ litellm_db_password }}"
            POSTGRES_DB: "{{ litellm_db_name }}"
          healthcheck:
            test: ["CMD-SHELL", "pg_isready -U {{ litellm_db_user }}"]
            interval: 5s
            timeout: 5s
            retries: 10
          restart: unless-stopped
          networks:
            - llm-network

        redis:
          image: "{{ litellm_redis_image }}"
          container_name: "{{ litellm_redis_container_name }}"
          ports:
            - "{{ litellm_redis_port_host }}:{{ litellm_redis_port_container }}"
          volumes:
            - "{{ litellm_redis_data_dir }}:/data"
          command: redis-server --appendonly yes
          restart: unless-stopped
          networks:
            - llm-network

        litellm-proxy:
          image: "{{ litellm_proxy_image }}"
          container_name: "{{ litellm_proxy_container_name }}"
          ports:
            - "{{ litellm_proxy_port_host }}:{{ litellm_proxy_port_container }}"
          volumes:
            - "{{ litellm_proxy_config_full_host_path }}:/app/config.yaml:ro"
          environment:
            LANGFUSE_PUBLIC_KEY: "{{ langfuse_homelab_public_key }}"
            LANGFUSE_SECRET_KEY: "{{ litellm_homelab_secret_key | default(langfuse_homelab_secret_key) }}"
            LANGFUSE_HOST: "{{ langfuse_host_for_litellm }}"
            LITELLM_MASTER_KEY: "{{ litellm_master_key }}"
            DATABASE_URL: "{{ litellm_database_url }}"
            REDIS_HOST: "{{ litellm_redis_container_name }}"
            REDIS_PORT: "{{ litellm_redis_port_container }}"
            GEMINI_API_KEY: "{{ GEMINI_API_KEY }}"
            OPENROUTER_API_KEY: "{{ OPENROUTER_API_KEY }}"
            ANTHROPIC_API_KEY: "{{ ANTHROPIC_API_KEY }}"
          command: ["--config", "/app/config.yaml", "--port", "{{ litellm_proxy_port_container }}"]
          depends_on:
            postgres:
              condition: service_healthy
            redis:
              condition: service_started
          restart: unless-stopped
          networks:
            - llm-network
          labels:
            wud.tag.include: main
            wud.watch.digest: "true"
            homepage.group: "{{ services['litellm-ui'].group }}"
            homepage.name: "{{ services['litellm-ui'].name }}"
            homepage.href: https://litellm-ui.lan
            homepage.icon: "{{ services['litellm-ui'].icon | default('litellm.png') }}"
            homepage.server: "{{ inventory_hostname }}"
            homepage.container: "{{ litellm_proxy_container_name }}"

        mcp-proxy:
          image: "{{ mcp_proxy_image_custom }}"
          container_name: "{{ mcp_proxy_container_name }}"
          ports:
            - "{{ mcp_proxy_service_port }}:{{ mcp_proxy_service_port }}"
          volumes:
            - "{{ mcp_proxy_config_full_host_path }}:/app/servers.json:ro"
          command: "mcp-proxy --port {{ mcp_proxy_service_port }} --host 0.0.0.0 --named-server-config /app/servers.json --pass-environment"
          restart: unless-stopped
          networks:
            - llm-network

        open-webui:
          image: ghcr.io/open-webui/open-webui:main
          container_name: open-webui
          ports:
            - "{{ open_webui_http_port }}:8080"
          environment:
            OLLAMA_BASE_URL: "{{ ollama_base_url_for_webui }}"
            WEBUI_SECRET_KEY: t0p-s3cr3t
            SSL_CERT_FILE: "/etc/ssl/certs/ca-certificates.crt"
          volumes:
            - open-webui:/app/backend/data
            - "/etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt:ro"
          labels:
            wud.tag.include: main
            wud.watch.digest: "true"
            homepage.group: "{{ services['open-webui'].group }}"
            homepage.name: "{{ services['open-webui'].name }}"
            homepage.href: https://open-webui.lan
            homepage.icon: openai.png
            homepage.server: "{{ inventory_hostname }}"
            homepage.container: open-webui
          restart: unless-stopped
          networks:
            - llm-network
          depends_on:
            litellm-proxy:
              condition: service_started
            mcp-proxy:
              condition: service_started

        google-workspace-mcp:
          image: "{{ google_workspace_mcp_image }}"
          container_name: "{{ google_workspace_mcp_container_name }}"
          ports:
            - "{{ google_workspace_mcp_port_host }}:{{ google_workspace_mcp_port_container }}"
          environment:
            GOOGLE_OAUTH_CLIENT_ID: "{{ google_workspace_oauth_client_id }}"
            GOOGLE_OAUTH_CLIENT_SECRET: "{{ google_workspace_oauth_client_secret }}"
            MCP_ENABLE_OAUTH21: "true"
            WORKSPACE_MCP_STATELESS_MODE: "true"
            WORKSPACE_EXTERNAL_URL: "https://google-workspace-mcp.net"
          command:
            - "uvx"
            - "workspace-mcp"
            - "--transport"
            - "streamable-http"
          restart: unless-stopped
          networks:
            - llm-network

      networks:
        llm-network:
          driver: bridge

      volumes:
        open-webui:
  become: true
